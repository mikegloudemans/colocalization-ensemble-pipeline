#!/bin/python3

import sys
import os
import glob

configfile:
	#"colocalization-config.json"
	"whr-test-config.json"

# change working directory
os.chdir(config['outdir'])
# make outdir and tmpdir 
subprocess.call('mkdir -p log/cluster', shell=True)
subprocess.call('mkdir -p {}'.format(config['tmpdir']), shell=True)

#########################################################################

# define colocalization methods
methods = [k for k in config['colocalization_methods']]

def get_trait_set(set_number):
	trait_set = set()
	for trait in config['studies']:
		if 'trait_set' not in config['studies'][trait]:
			continue
		if config['studies'][trait]['trait_set'] == set_number:
			trait_set.add(trait)
	return(trait_set)

# assume there are two sets of traits for now
# all studies in trait_set_1 are tested against all studies in trait_set_2
# we should probably also add the option to provide a two-column input of all desired trait comparisons

trait_set_1 = get_trait_set(1)
trait_set_2 = get_trait_set(2)
studies = list(chain(*[list(trait_set_1), list(trait_set_2)]))

# don't let wildcards contain directory
wildcard_constraints:
    study="[^/]*",
    trait1="[^/]*",
    trait2="[^/]*",
    method="[^/]*"


# make dictionary of all traits to file paths 
trait_to_raw_file = {}
for trait in studies:
	trait_to_raw_file[trait] = config['studies'][trait]['file']

if len(trait_set_1) == 0 or len(trait_set_2) == 0:
	sys.exit('Two sets of traits were not defined in the config file. (more detail...)')


# rules that can be run on the login node
localrules: all,stage

# here we define colocalization methods and trait combinations 
rule all:
	input:
		expand('results/{method}/TRAIT1-{trait1}.TRAIT2-{trait2}.METHOD-{method}.results.txt', 
			trait1 = trait_set_1, 
			trait2 = trait_set_2,
			method = methods),
		expand('preprocess/{study}.formatted.summary_stats.txt.gz',
			study = studies)

rule stage:
	input:
		lambda wildcards: trait_to_raw_file[wildcards.study]
	output:
		'summary_stats/{study}.raw.summary_stats.txt.gz'
	shell:
		'''
		ln -s {input} {output}
		ln -s {input}.tbi {output}.tbi
		'''

rule preprocess:
	input:
		'summary_stats/{study}.raw.summary_stats.txt.gz'
	output:
		'preprocess/{study}.formatted.summary_stats.txt.gz'
	shell:
		'''
		# Dummy code until we actually write the preprocess rule;
		# Right now I'm just using stuff that's already been preprocessed.
		ln -s ../summary_stats/{wildcards.study}.raw.summary_stats.txt.gz preprocess/{wildcards.study}.formatted.summary_stats.txt.gz
		ln -s ../summary_stats/{wildcards.study}.raw.summary_stats.txt.gz.tbi preprocess/{wildcards.study}.formatted.summary_stats.txt.gz.tbi
		'''

rule get_gwas_hits:
	input:
		'preprocess/{trait1}.formatted.summary_stats.txt.gz'
	output:
		'gwas_top_hits/{trait1}.hits.txt'
	params:
		pvalue_threshold = config["overlap_settings"]["selection_thresholds"]["gwas"],
		window = config["overlap_settings"]["selection_separation_window"]

	shell:
		'python ../scripts/overlap/get_gwas_hits.py {input} {output} {params.pvalue_threshold} {params.window}'

rule overlap:
	input:
		trait1 = 'gwas_top_hits/{trait1}.hits.txt',
		trait2 = 'preprocess/{trait2}.formatted.summary_stats.txt.gz'
	output:
		'overlap/TRAIT1-{trait1}.TRAIT2-{trait2}.overlap.txt'
	params:
		pvalue_threshold = config["overlap_settings"]["selection_thresholds"]["eqtl"],	
		window = config["overlap_settings"]["lookup_window"]

	shell:
		'python ../scripts/overlap/list_snps_to_test.py {input.trait1} {input.trait2} {output} {params.pvalue_threshold} {params.window}'

rule process_loci:
	input:
		'overlap/TRAIT1-{trait1}.TRAIT2-{trait2}.overlap.txt'
	output:
		'process_loci/TRAIT1-{trait1}.TRAIT2-{trait2}.checkpoint.txt'
	params:
		source_file = lambda x: config["studies"][x.trait1]["file"],
		lookup_file = lambda x: config["studies"][x.trait2]["file"],
		window = config["colocalization_settings"]["window"]
	shell:
		'''
		# preprocess each locus and output merged data to file with predictable name
		# we aren't actually going to tell snakemake about this directory 
		mkdir -p process_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}

		i=0
		tail -n +2 {input} | while read chrom pos source_pvalue source_trait lookup_pvalue lookup_trait; do
			let i=i+1
			# each file in this directory should be the merged summary stats for a locus
			# here we just touch a dummy file
		
			python ../scripts/process_loci/preprocess.py process_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}/locus.${{i}}.txt {params.window} {params.source_file} {params.lookup_file} ${{chrom}} ${{pos}} ${{source_trait}} ${{lookup_trait}}
	
		done 
		#< {input}

		# when done:
		echo '{wildcards.trait1} {wildcards.trait2}' > {output}
		'''


rule coloc:
	input:
		'process_loci/TRAIT1-{trait1}.TRAIT2-{trait2}.checkpoint.txt'
	output:
		'results/coloc/TRAIT1-{trait1}.TRAIT2-{trait2}.METHOD-{method}.results.txt'
	params: 
	shell:
		'''
		# do something for each file in processed_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}/
		for locus in $(ls process_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}/); do
			echo ${{locus}} >> {output}
		done
		'''


rule finemap:
	input:
		'process_loci/TRAIT1-{trait1}.TRAIT2-{trait2}.checkpoint.txt'
	output:
		'results/finemap/TRAIT1-{trait1}.TRAIT2-{trait2}.METHOD-{method}.results.txt'
	params: 
	shell:
		'''
		# do something for each file in processed_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}/
		for locus in $(ls process_loci/TRAIT1-{wildcards.trait1}.TRAIT2-{wildcards.trait2}/); do
			echo ${{locus}} >> {output}
		done
		'''

